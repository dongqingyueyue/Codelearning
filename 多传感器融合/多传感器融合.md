[toc]

|  作者   | 版本 |      联系方式      |
| :-----: | :--: | :----------------: |
| yangdq6 | 1.0  | yangdq6@lenovo.com |

# <center>多传感器融合</center>

## 1.简介

**多传感器融合**（Multi-sensor Fusion, MSF）是利用计算机技术，将来自多传感器或多源的信息和数据以一定的准则进行自动分析和综合，以完成所需的决策和估计而进行的信息处理过程。

多传感器融合基于多种传感器的特性进行感知和信息融合，常见到的传感器类型为：

![image-20200713110053289](D:\多传感器融合\image-20200713110053289.png)

多传感器融合要求：

1 ）硬件层面：数量要足够，也就是不同种类的传感器都要配备，才能够保证信息获取充分且有冗余；

2 ）软件层面：算法要足够优化，数据处理速度要够快，且容错性要好，才能保证最终决策的快速性和正确性。

## 2.分类

**多传感器融合的体系结构**

多传感器融合的体系结构：**分布式、集中式和混合式。**

1）**分布式**：先对各个独立传感器所获得的原始数据进行局部处理，然后再将结果送入信息融合中心进行智能优化组合来获得最终的结果。分布式对通信带宽的需求低、计算速度快、可靠性和延续性好，但跟踪的精度却远没有集中式高。

2 ）**集中式**：集中式将各传感器获得的原始数据直接送至中央处理器进行融合处理，可以实现实时融合。其数据处理的精度高，算法灵活，缺点是对处理器的要求高，可靠性较低，数据量大，故难于实现。

3）**混合式**：混合式多传感器信息融合框架中，部分传感器采用集中式融合方式，剩余的传感器采用分布式融合方式。混合式融合框架具有较强的适应能力，兼顾了集中式融合和分布式的优点，稳定性强。混合式融合方式的结构比前两种融合方式的结构复杂，这样就加大了通信和计算上的代价。

![image-20200713180136685](D:\多传感器融合\多传感器融合分类.png)

 **多传感器融合的先决条件**

1.统一时钟

主要目的是同步不同传感器的时间戳，保证有一个相同的时间系统；

不同传感器的数据频率是不同的，数据还存在延迟，解决方法：

​    1.可以通过找相邻时间戳的方法找到最近帧

​    2.硬件同步大大降低同步误差，提高数据对齐效果。

2.统一坐标系

​    统一坐标系有两步，**一是运动补偿，二是传感器标定**。

**运动补偿**主要针对长周期的传感器，如lidar，周期为100ms。由于所有的传感器都装在车上，车是运动的刚体。因此传感器在采集数据时，周期开始的时间点和结束时间点车辆是处于不同位置的，导致不同时刻采集的数据所处坐标系不同，因此需要根据车体的运动对传感器采集的数据进行运动补偿。

**传感器标定**分为内参标定和外参标定，内参标定，解决的是单独的每个传感器与世界坐标系间的变换；外参标定是在世界坐标系下，解决的不同传感器间的变换。传感器外参校准依赖于传感器的精确内参校准。

## 3.行业

多传感器融合应用领域比较多的领域有：自动驾驶、机器人、AR/VR、无人飞行器、室内定位等。

### 1.自动驾驶

自动驾驶是涉及多传感器融合比较多的领域，也是比较复杂的领域。

自动驾驶产业链分析：

 [自动驾驶产业链.pdf](自动驾驶产业链.pdf) 

![自动驾驶算法](D:\多传感器融合\自动驾驶算法.jpg)

上图是整理的视觉传感器和激光传感器在自动驾驶算法中使用的算法思路。

Appllo 、google和Stanford无人车实验室的定位方案是：GNSS/INS/LiDAR/HDmap融合定位方案，利用实时采集的点云与已有的高精地图点云进行匹配，基本框架都是基于贝叶斯滤波框架，具体方案无法了解，百度有篇论文介绍了LiDAR/camera/HDmap 融合定位的方法。

Stanford 2010论文： [Robust Vehicle Localization in urbanEnvironments using  probabilistic maps.pdf](Robust Vehicle Localization in urbanEnvironments using  probabilistic maps.pdf) 

以特斯拉（Tesla）、Mobileye为首的视觉定位方案：GNSS/INS/camera/HDmap融合定位方案。

### 2.机器人&AGV

####2.1机器人



![image-20200713173933121](D:\多传感器融合\robot.png)

![机器人地盘](D:\多传感器融合\机器人地盘.jpg)

![image-20200713174053508](D:\多传感器融合\用于开发的机器人.png)

#### 2.2 AGV简介

AGV为Automated Guided Vehicle的缩写，其装备自动引导装置，在计算机监控下能够沿着规定的导引路径行驶。AGV是轮式移动机器人（WMR-wheel  Mobile Robot）的特殊应用。

![image-20200713141200136](D:\多传感器融合\image-20200713141200136.png)

![image-20200713141925510](D:\多传感器融合\软件架构.png)



![image-20200713141310106](D:\多传感器融合\image-20200713141310106.png)

![image-20200713141354945](D:\多传感器融合\引导方式.png)

![image-20200713141459145](D:\多传感器融合\电磁引导.png)

![image-20200713141546384](D:\多传感器融合\磁引导原理.png)

![image-20200713143857690](D:\多传感器融合\磁带引导.png)

![image-20200713144004126](D:\多传感器融合\惯性引导.png)

![image-20200713144115001](D:\多传感器融合\激光引导.png)

![image-20200713144223775](D:\多传感器融合\激光反光板.png)

![image-20200713144317017](D:\多传感器融合\激光引导的应用.png)

![image-20200713144506256](D:\多传感器融合\视觉引导.png)

![image-20200713162419133](D:\多传感器融合\导航方式比较.png)

SLAM运用在AGV上，可以不用预先铺设任何轨道，方便工厂生产线的升级改造和导航路线的变更，实时避障，环境适应能力强，更好地实现多AGV小车的协调控制，实现在未知环境中进行自主定位和导航。AGV中多传感器融合其实就是SLAM系统。

AGV中进行环境信息感知的主要传感器有激光雷达、摄像头等。基于激光雷达的AGV小车SLAM，由于其测量精度高探测距离远，且不易受外界环境的干扰，是目前AGV市场上采用的主要方案，而基于视觉的AGV小车SLAM依靠其成本的优势加之人工智能的独特优势，已经成为当前AGV小车的研究热点。

#####2.2.1基于激光雷达的AGV小车SLAM方案

基于激光雷达的AGV小车SLAM方案基于的开源算法：

![image-20200713150240737](D:\多传感器融合\激光slam.png)

激光AGV算法:

<video src="D:\多传感器融合\机器人\ROS - Google Cartographer, Hector SLAM and GMapping SLAM.mp4"></video>

主流的激光SLAM算法有hector、gmapping、karto、cartographer。

hector是一种结合了鲁棒性较好的扫描匹方法2D_SLAM方法和使用惯性传感系统的导航技术。传感器的要求较高，高更新频率小测量噪声的激光扫描仪，不需要里程计。使空中无人机与地面小车在不平坦区域运行存在运用的可能性。作者利用现代激光雷达的高更新率和低距离测量噪声，通过扫描匹配实时地对机器人运动进行估计。所以当只有低更新率的激光传感器时，即便测距估计很精确，对该系统都会出现一定的问题。

gmapping是一种基于粒子滤波的激光SLAM算法，它已经集成在ROS中，是移动机器人中使用最多的SLAM算法。基于粒子滤波的算法用许多加权粒子表示路径的后验概率，每个粒子都给出一个重要性因子。但是，它们通常需要大量的粒子才能获得比较好的的结果，从而增加该算法的的计算复杂性。此外，与PF重采样过程相关的粒子退化耗尽问题也降低了算法的准确性。

karto是基于图优化的SLAM算法，用高度优化和非迭代cholesky矩阵进行稀疏系统解耦作为解。图优化方法利用图的均值表示地图，每个节点表示机器人轨迹的一个位置点和传感器测量数据集，箭头的指向的连接表示连续机器人位置点的运动，每个新节点加入，地图就会依据空间中的节点箭头的约束进行计算更新。路标landmark越多,内存需求越大,然而图优化方式相比其他方法在大环境下制图优势更大。

cartographer是google开发的实时室内SLAM项目，cartographer采用基于google自家开发的ceres非线性优化的方法，cartographer的量点在于代码规范与工程化，非常适合于商业应用和再开发。并且cartographer基于submap子图构建全局地图的思想，能有效的避免建图过程中环境中移动物体的干扰。并且cartographer支持多传感器数据（odometry、IMU、LaserScan等）建图，支持2D_SLAM和3D_SLAM建图。

cartographer处理框架

cartographer采用的是主流的SLAM框架，也就是特征提取、闭环检测、后端优化的三段式。由一定数量的LaserScan组成一个submap子图，一系列的submap子图构成了全局地图。用LaserScan构建submap的短时间过程累计误差不大，但是用submap构建全局地图的长时间过程就会存在很大的累计误差，所以需要利用闭环检测来修正这些submap的位置，闭环检测的基本单元是submap，闭环检测采用scan_match策略。cartographer的重点内容就是融合多传感器数据（odometry、IMU、LaserScan等）的submap子图创建以及用于闭环检测的scan_match策略的实现。

![image-20200715163108974](D:\多传感器融合\cartographer框架.png)

 [20200510-崔佳峰-Cartographer代码讲解cartographer_ros.pdf.pdf](cartographer\20200510-崔佳峰-Cartographer代码讲解cartographer_ros.pdf.pdf)

 [2020.06.28-激光SLAM前端-张涵.pdf](cartographer\2020.06.28-激光SLAM前端-张涵.pdf) 

 [激光SLAM数据预处理-曹秀洁 - 20200524.pdf](cartographer\激光SLAM数据预处理-曹秀洁 - 20200524.pdf) 

 [张涵cartographer论文带读.pdf](cartographer\张涵cartographer论文带读.pdf) 

[Real Time Loop Closure in 2D LIDAR SLAM.pdf](cartographer\Real Time Loop Closure in 2D LIDAR SLAM.pdf) 

#####2.2.2基于视觉的AGV小车SLAM方案

基于视觉的AGV小车SLAM技术方案各个AGV厂商都处于研究阶段，各家方案并没有详细资料，目前搜到的资料只有一家马路创新的公司宣称基于视觉SLAM AGV小车落地 。

视觉SLAM的方案大都基于开源的视觉SLAM算法，如下是网上搜到整理的开源方案：

[视觉开源SLAM代码.pdf](视觉开源SLAM代码.pdf) 

1.msckf --滤波框架（rovio也是采用的滤波框架）

MSCKF的目标是解决EKF-SLAM的维数爆炸问题。传统EKF-SLAM将特征点加入到状态向量中与IMU状态一起估计，当环境很大时，特征点会非常多，状态向量维数会变得非常大。MSCKF不是将特征点加入到状态向量，而是将不同时刻的相机位姿(位置 ![[公式]](https://www.zhihu.com/equation?tex=p)和姿态四元数 ![[公式]](https://www.zhihu.com/equation?tex=q))加入到状态向量，特征点会被多个相机看到，从而在多个相机状态（Multi-State）之间形成几何约束（Constraint），进而利用几何约束构建观测模型对EKF进行update。由于相机位姿的个数会远小于特征点的个数，MSCKF状态向量的维度相较EKF-SLAM大大降低，历史的相机状态会不断移除，只维持固定个数的的相机位姿（Sliding Window），从而对MSCKF后端的计算量进行限定。

![image-20200716134707633](D:\多传感器融合\s-MSCKF.png)

![image-20200716134801498](D:\多传感器融合\msckf框架.png)

 [msckf_note_cg.pdf](msckf_note_cg.pdf) 

http://www.xinliang-zhong.vip/msckf_notes/  

https://zhuanlan.zhihu.com/p/76341809

2.okvis 2.0 优化框架

 [smart-robotics-lab.pdf](smart-robotics-lab.pdf) 

轮速计的工作原理

根据安装在双轮差动机器人左右两个驱动轮电机上的广电编码器，来检测车轮在一定时间内移动的距离，从而推算出机器人相对位姿（位置和航向）的变化

![image-20200723170944524](D:\多传感器融合\image-20200723170944524.png)

![image-20200723171024372](D:\多传感器融合\轮速计姿态.png)

[轮速仪的建模]([https://blog.csdn.net/KYJL888/article/details/100515455?biz_id=102&utm_term=%E8%BD%AE%E9%80%9F%E8%AE%A1%E7%9A%84%E5%BB%BA%E6%A8%A1%E4%B8%8E%E7%B2%BE%E5%BA%A6%E8%AF%84%E4%BC%B0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~sobaiduweb~default-1-100515455&spm=1018.2118.3001.4187](https://blog.csdn.net/KYJL888/article/details/100515455?biz_id=102&utm_term=轮速计的建模与精度评估&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~sobaiduweb~default-1-100515455&spm=1018.2118.3001.4187))



##Visual-Inertial Odometry Tightly Coupled with wheel Encoder Adopting  Robust Initialization and online Extrinsic Calibration

IMU-odometer预积分

假设轮速传感器只安装在单轮上，比如左后轮。

在VINS的预积分的基础上进行扩展，使用陀螺和编码器的读数构建编码器的预积分方程。

![image-20200729155835751](D:\多传感器融合\image-20200729155835751.png)

其中第四行代表里程计的位移。

协方差的推导如下：

![image-20200729160701633](D:\多传感器融合\里程计.png)

里程计的误差主要有陀螺仪的误差和白噪声引起，残差方程：

![image-20200729165124128](D:\多传感器融合\里程计残差.png)

使用里程计初始化方法：

VINS常速度初始化面临的问题可以添加里程计来解决，方法：

使用视觉来求两关键帧之间的旋转，从而求出陀螺的零偏。![image-20200729165209213](D:\多传感器融合\gyrobias.png)

优化重力：

![image-20200729165256111](D:\多传感器融合\refineG.png)



如果是双轮里程计：

通过双轮里程计可以获取平面坐标、航向角。

方案1：右轮采用和左轮同样的方案，右轮的坐标系？

方案二：假设平面运动

1.视觉的旋转矩阵与里程计的旋转矩阵？？

2.imu与视觉的残差？

3.平面残差

4.重投影误差

## Thghtly-coupled Monocular visual-odometric SLAM using wheels and a MEMS Gyroscope

